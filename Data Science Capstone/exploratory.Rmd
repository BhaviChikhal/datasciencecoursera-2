---
title: "Capstone Swiftkey Exploratory Analysis"
output: html_document
---

##Introduction

The goal of the project is to be able to predict the words that follow from it's preceding text. The data comes from three sources: blogs, twitter, and the news.

Anyways, the english data is first loaded.

```{r}
#Packages
library(ggplot2)
library(tm)

#Data
dir = 'input/en_US/'
blogs = readLines(paste0(dir,"en_US.blogs.txt"))
twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
news = readLines(con, encoding="UTF-8")
close(con)
```


```{r}
data.frame(dataset=c("blogs", "news", "twitter"), num_lines=(c(length(blogs), length(news),length(twitter))), max_length=c(nchar(blogs[which.max(nchar(blogs))]), nchar(news[which.max(nchar(news))]), nchar(twitter[which.max(nchar(twitter))])))
```
As can be seen in this data frame, twitter seems to have the largest number of elements compared to the other datasets. Blogs has the longest length followed by news. The longest tweet is a bit under the 213 which is odd considering the max length of 140. The data will be sampled for training data.

```{r}
#Takes sampling of each data
train_blogs = sample(blogs, 100000)
train_news = sample(news, 100000)
train_twitter = sample(twitter, 100000)
```

```{r}
#Combine the datasets and begin to create the corpus of words
combined_raw = c(train_blogs,  train_news, train_twitter)

corpus = Corpus(VectorSource(combined_raw))
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
#consider stemming
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument


#Convert to DocumentTermMatrix and remove sparse terms
dtm = DocumentTermMatrix(corpus)

common = removeSparseTerms(dtm, 0.95) #keep words in that appear in 95%
train_words = as.data.frame(as.matrix(common))

```