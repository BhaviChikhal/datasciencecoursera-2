---
title: "Capstone Swiftkey Exploratory Analysis"
output: html_document
---

##Introduction

The goal of the project is to be able to predict the words that follow from it's preceding text. The data comes from three sources: blogs, twitter, and the news.

Anyways, the english data is first loaded.

```{r}
#Packages
library(ggplot2)
library(tm)
library(wordcloud)
library(RWeka)

#Data
dir = 'input/en_US/'
blogs = readLines(paste0(dir,"en_US.blogs.txt"))
twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
news = readLines(con, encoding="UTF-8")
close(con)
```


```{r}
data.frame(dataset=c("blogs", "news", "twitter"), num_lines=(c(length(blogs), length(news),length(twitter))), max_length=c(nchar(blogs[which.max(nchar(blogs))]), nchar(news[which.max(nchar(news))]), nchar(twitter[which.max(nchar(twitter))])))
```
As can be seen in this data frame, twitter seems to have the largest number of elements compared to the other datasets. Blogs has the longest length followed by news. The longest tweet is a bit under the 213 which is odd considering the max length of 140. The data will be sampled for training data.

```{r}
#Takes sampling of each data
train_blogs = sample(blogs, 50000)
train_news = sample(news, 50000)
train_twitter = sample(twitter, 50000)
```

```{r}
#Combine the datasets and begin to create the corpus of words
combined_raw = c(train_blogs,  train_news, train_twitter)

corpus = Corpus(VectorSource(combined_raw))
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument


#Convert to DocumentTermMatrix and remove sparse terms
ngram1 = DocumentTermMatrix(corpus)

ngram1 = removeSparseTerms(dtm, 0.995) #keep words in that appear in 99.5%
ngram1 = as.data.frame(as.matrix(common))

#Word cloud of the most frequent words
word_freqs = sort(colSums(ngram1), decreasing=TRUE) 
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"), max.words=100)
```

As can be seen from the word cloud, the most frequent 1grams are stop words. It makes sense as in everyday speech, those words are the most important in order to connect our sentences and make logical sense. There are other common words including "said", "tell", "love", and others which makes sense as the data comes from news as well as from blogs and tweets.


```{r}
#Create n-grams of 2
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngram2 = TermDocumentMatrix(corpus, control = list(tokenize = TwogramTokenizer))
ngram2 = removeSparseTerms(ngram2, 0.995)

#Word cloud of frequent 2grams
m = as.matrix(ngram2)
word_freqs = sort(rowSums(m), decreasing=TRUE) 
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
```

Like the previous case with the most common words, the 2grams appears to be conneting statements. In addition to the connceting statements, there are those such as to do and other verb phrases. There are also descriptive statements such as "the us."

```{r}
#Create n-gram of 3
TrigramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ngram3 = TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
ngram3 = removeSparseTerms(ngram3, 0.999)

#Word cloud of frequent 3gram
m = as.matrix(ngram3)
word_freqs = sort(rowSums(m), decreasing=TRUE) 
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
```

Similar to the others, these 3grams are utilize a lot fo connecting phrases. Some fo the three grams are common in everyday speech such as connecting phrases, what one wants to do or say, or common locales like the united states.