#Packages
library(ggplot2)
library(tm)
library(wordcloud)
library(RWeka)
#Data
dir = 'input/en_US/'
blogs = readLines(paste0(dir,"en_US.blogs.txt"))
twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
news = readLines(con, encoding="UTF-8")
close(con)
data.frame(dataset=c("blogs", "news", "twitter"), num_lines=(c(length(blogs), length(news),length(twitter))), max_length=c(nchar(blogs[which.max(nchar(blogs))]), nchar(news[which.max(nchar(news))]), nchar(twitter[which.max(nchar(twitter))])))
#Takes sampling of each data
train_blogs = sample(blogs, 50000)
train_news = sample(news, 50000)
train_twitter = sample(twitter, 50000)
#Combine the datasets and begin to create the corpus of words
combined_raw = c(train_blogs,  train_news, train_twitter)
corpus = Corpus(VectorSource(combined_raw))
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument
#Convert to DocumentTermMatrix and remove sparse terms
ngram1 = DocumentTermMatrix(corpus)
ngram1 = removeSparseTerms(ngram1, 0.995) #keep words in that appear in 99.5%
ngram1 = as.data.frame(as.matrix(common))
#Word cloud of the most frequent words
word_freqs = sort(colSums(ngram1), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"), max.words=100)
#Create n-grams of 2
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngram2 = TermDocumentMatrix(corpus, control = list(tokenize = TwogramTokenizer))
#Packages
library(ggplot2)
library(tm)
library(wordcloud)
library(RWeka)
options(warn=0)
#Packages
library(ggplot2)
library(tm)
library(wordcloud)
library(RWeka)
#Packages
library(ggplot2)
library(tm)
library(wordcloud)
library(RWeka)
#Data
dir = 'input/en_US/'
blogs = readLines(paste0(dir,"en_US.blogs.txt"))
twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
news = readLines(con, encoding="UTF-8")
close(con)
#Takes sampling of each data
train_blogs = sample(blogs, 50000)
train_news = sample(news, 50000)
train_twitter = sample(twitter, 50000)
#Combine the datasets and begin to create the corpus of words
combined_raw = c(train_blogs,  train_news, train_twitter)
corpus = Corpus(VectorSource(combined_raw))
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument
#Convert to DocumentTermMatrix and remove sparse terms
ngram1 = DocumentTermMatrix(corpus)
ngram1 = removeSparseTerms(ngram1, 0.995) #keep words in that appear in 99.5%
ngram1 = as.data.frame(as.matrix(ngram1))
#Word cloud of the most frequent words
word_freqs = sort(colSums(ngram1), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"), max.words=100)
#Create n-grams of 2
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngram2 = TermDocumentMatrix(corpus, control = list(tokenize = TwogramTokenizer))
ngram2 = removeSparseTerms(ngram2, 0.995)
#Word cloud of frequent 2grams
m = as.matrix(ngram2)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
#Create n-gram of 3
TrigramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ngram3 = TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
ngram3 = removeSparseTerms(ngram3, 0.999)
#Word cloud of frequent 3gram
m = as.matrix(ngram3)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
warnings()
head(dm)
hist(dm)
ggplot(dm, aes(x=word, y=freq)) + geom_histogram()
ggplot(dm, aes(x=word, y=freq)) + geom_histogram(stat=identity)
ggplot(dm, aes(x=word, y=freq)) + geom_histogram(stat="identity")
head(dm[sort(dm),])
head(dm[,sort(dm)])
head(dm[,order(dm)])
order(d,)
order(dm)
head(dm[order(dm)])
head(dm[order(dm),])
length(dm)
dim(dm)
?order
?order
sort.list(dm)
order(dm)
order(dm$freq)
head(dm[order(dm$freq),])
head(dm[order(dm$freq, decresing=TRUE),])
head(dm[order(dm$freq, decreasing=TRUE),])
ordered= dm[order(dm$freq, decreasing=TRUE),]
ggplot(ordered[1:10], aes(x=word, y=freq)) + geom_histogram(stat="identity")
ggplot(ordered, aes(x=word, y=freq)) + geom_histogram(stat="identity")
ordered= dm[order(dm$freq, decreasing=TRUE),]
ordered = ordered[1:20]
ordered[1:20]
ordered[1:20,]
ordered = ordered[1:20,]
ggplot(ordered, aes(x=word, y=freq)) + geom_histogram(stat="identity")
??labeller
??label
ordered= dm[order(dm$freq, decreasing=TRUE),]
ordered = ordered[1:10,]
ggplot(ordered, aes(x=word, y=freq)) + geom_histogram(stat="identity")
word_freqs = sort(colSums(ngram1), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
ordered= dm[order(dm$freq, decreasing=TRUE),]
ordered = ordered[1:10,]
ggplot(ordered, aes(x=word, y=freq)) + geom_histogram(stat="identity") + labs(title="Top 10 1gram")
m = as.matrix(ngram2)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
ordered= dm[order(dm$freq, decreasing=TRUE),]
ordered = ordered[1:10,]
ggplot(ordered, aes(x=word, y=freq)) + geom_histogram(stat="identity") + labs(title="Top 10 2gram")
