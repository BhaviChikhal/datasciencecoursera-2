#Packages
library(ggplot2)
#Data
dir = 'input/en_US/'
train_blogs = read.table(paste0(dir,"en_US.blogs.txt"))
train_news = read.table(paste0(dir,"en_US.news.txt"))
train_twitter = read.table(paste0(dir,"en_US.twitter.txt"))
train_blogs
getwd()
dir()
train_blogs = read(paste0(dir,"en_US.blogs.txt"))
?read.table
train_blogs = read.table(paste0(dir,"en_US.blogs.txt"), header = FALSE)
train_blogs = read.table(paste0(dir,"en_US.blogs.txt"), header = FALSE, allowEscapes = TRUE)
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"), header = FALSE)
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"))
head(train_blogs)
dir = 'input/en_US/'
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"))
?readLines
train_news = readLines(paste0(dir,"en_US.news.txt"))
train_twitter = readLines(paste0(dir,"en_US.twitter.txt"))
head(train_news)
head(train-tiwtter)
head(train_tiwtter)
head(train_twitter)
rbinom(1)
rbinom(1,1)
?rbinom
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
rbinom(1,2, .5)
?rbinom
rbinom(1,2, .5)
rbinom(1,2, .5)
?rbinom
rbinom(1,2, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(0,1, .5)
rbinom(1,1, .5)
rbinom(1,1, .5)
rbinom(1,1, .5)
rbinom(1,1, .5)
rbinom(1,1, .5)
rbinom(1,1, .5)
rbinom(1,1, .5)
train_news = readLines(paste0(dir,"en_US.news.txt"))
?readLines
train_news = readLines(paste0(dir,"en_US.news.txt"), skipNul = TRUE)
length(train_blogs)
sample(train_blogs)
?sample
train_blogs = sample(train_blogs, 300000)
train_twitter = sample(train_twitter, 300000)
train_news = readLines(paste0(dir,"en_US.news.txt"))
?scan
train_news = scan(paste0(dir,"en_US.news.txt"))
con <- file(paste0(dir,"en_US.blogs.txt"), "r")
readLines(con, 1)
close(con)
news <- file(paste0(dir,"en_US.news.txt"), "r")
train_news = readLines(news, 77259) #Seems to be missing new line char at 77260
tail(train_news)
train_news = readLines(news, 77258) #Seems to be missing new line char at 77258
head(train_news)
readLines(news, 1) #Seems to be missing new line char at 77258
close(con)
close(news)
news <- file(paste0(dir,"en_US.news.txt"), "r")
train_news = readLines(news, 77258) #Seems to be missing new line char at 77258
close(news)
news <- file(paste0(dir,"en_US.news.txt"), "r")
train_news = readLines(news, 77258) #Seems to be missing new line char at 77258
readLines(news, 2)
Sys.setlocale(category="LC_ALL", locale = "English_United States.1252")
train_news = readLines(paste0(dir,"en_US.news.txt"))
tail(train_news)
train_news = readLines(paste0(dir,"en_US.news.txt"))
tail(train_news)
train_news = readLines(paste0(dir,"en_US.news.txt"))
train_blogs[1:nrow(train_blogs) - 1]
nrow(train_blogs)
length(train_news)
con = file(paste0(dir,"en_US.news.txt"), open="rb")
train_news <- readLines(con, encoding="UTF-8")
close(con)
?file
dir = 'input/en_US/'
con = file(paste0(dir,"en_US.news.txt"), open="rb")
train_news = readLines(con, encoding="UTF-8")
close(con)
#Packages
library(ggplot2)
#Data
dir = 'input/en_US/'
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"))
train_twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
train_news = readLines(con, encoding="UTF-8")
close(con)
#Packages
library(ggplot2)
#Data
dir = 'input/en_US/'
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"))
train_twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
train_news = readLines(con, encoding="UTF-8")
close(con)
train_blogs = sample(train_blogs, 300000)
train_news = sample(train_news, 300000)
train_twitter = sample(train_twitter, 300000)
library(tm)
install.packages("tm")
library(tm)
rpaste?
combined_raw = c(train_blogs,  train_news, train_twitter)
?DocumentTermMatrix
dtm_raw = DocumentTermMatrix(combined_raw)
?vector
dtm_raw = DocumentTermMatrix(combined_raw)
?vector
dtm_raw = DocumentTermMatrix(combined_raw)
?DocumentTermMatrix
?Corpus
dtm_raw = VCorpus(VectorSource(combined_raw))
dtm_raw
corpus = VCorpus(VectorSource(combined_raw))
corpus = tm_map(corpus, removeNumbers) # removing numbers
corpus = tm_map(corpus, stripWhitespace) # removing whitespaces
corpus = tm_map(corpus, tolower) #lowercasing all contents
corpus = tm_map(corpus, removePunctuation) # removing special characters
corpus
head(combined_raw)
#Packages
library(ggplot2)
library(tm)
#Data
dir = 'input/en_US/'
blogs = readLines(paste0(dir,"en_US.blogs.txt"))
twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
news = readLines(con, encoding="UTF-8")
close(con)
data.frame(dataset=c("blogs", "news", "twitter"), num_lines=(c(length(blogs), length(news),length(twitter))), max_length=c(nchar(blogs[which.max(nchar(blogs))]), nchar(news[which.max(nchar(news))]), nchar(twitter[which.max(nchar(twitter))])))
#Takes sampling of each data
train_blogs = sample(blogs, 100000)
train_news = sample(news, 100000)
train_twitter = sample(twitter, 100000)
#Combine the datasets and begin to create the corpus of words
combined_raw = c(train_blogs,  train_news, train_twitter)
corpus = Corpus(VectorSource(combined_raw))
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
#consider stemming
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
#Convert to DocumentTermMatrix and remove sparse terms
dtm = DocumentTermMatrix(corpus)
common = removeSparseTerms(dtm, 0.95) #keep words in that appear in 95%
train_words = as.data.frame(as.matrix(common))
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument
#Convert to DocumentTermMatrix and remove sparse terms
dtm = DocumentTermMatrix(corpus)
common = removeSparseTerms(dtm, 0.95) #keep words in that appear in 95%
train_words = as.data.frame(as.matrix(common))
head(train_words)
train_words
dtm
common = removeSparseTerms(dtm, 0.99) #keep words in that appear in 99%
train_words = as.data.frame(as.matrix(common))
head(train_words)
common = removeSparseTerms(dtm, 0.995) #keep words in that appear in 99.5%
train_words = as.data.frame(as.matrix(common))
freq = colSums(dtm)
freq = colSums(train_words)
freq
freq[order(freq)]
install.packages('wordcloud')
library(wordcloud)
words <- names(freq)
wordcloud(words[1:100], freq[1:100])
wordcloud(words[1:20], freq[1:20])
wordcloud(words[1:50], freq[1:50])
?wordcloud
wordcloud(words[1:50], freq[1:50], random.color = TRUE)
wordcloud(words[1:50], freq[1:50], random.color = TRUE)
wordcloud(words[1:50], freq[1:50], random.color = TRUE)
?wordcloud
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue"))
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue", "green"))
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue", "green"))
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue", "green"))
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue", "green"))
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue", "green"))
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue"))
?wordcloud
wordcloud(words[1:50], freq[1:50], colors=c("black", "blue"), ordered.colors = TRUE)
wordcloud(words[1:50], freq[1:50], random.order = TRUE)
wordcloud(words[1:50], freq[1:50], random.order = FALSE)
?wordcloud
wordcloud(words[1:50], freq[1:50], random.order = FALSE, colors=c("black", "blue", "green", "red"))
wordcloud(words, freq, random.order = FALSE, colors=c("black", "blue", "green", "red"), max.words=50)
wordcloud(words, freq, random.order = FALSE, colors=c("black", "blue", "green", "red"), max.words=50, scale=c(8,.3))
wordcloud(words, freq, random.order = FALSE, colors=c("black", "blue", "green", "red"), max.words=50, scale=c(8,.2))
wordcloud(words, freq, random.order = FALSE, colors=c("black", "blue", "green", "red"), max.words=50, scale=c(8,.3))
wordcloud(words, freq, random.order = TRUE, colors=c("black", "blue", "green", "red"), max.words=50, scale=c(8,.3))
wordcloud(words, freq, random.order = FALSE, colors=c("black", "blue", "green", "red"), max.words=50, scale=c(8,.3))
intsall.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
pal2 <- brewer.pal(8,"Dark2")
wordcloud(words, freq, random.order = FALSE, colors=pal2, max.words=50, scale=c(8,.3))
?brewer.pal
pal = brewer.pal(8, "BuGn")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(8, "GnBu")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(8, "YlGn")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(8, "YlGn")
pal = brewer.pal(12, "YlGn")
pal = brewer.pal(9, "YlGn")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(5, "YlGn")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(2, "YlGn")
pal = brewer.pal(4, "YlGn")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(4, "PuBu")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(8, "PuBu")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
pal = brewer.pal(5, "Dark2")
wordcloud(words, freq, random.order = FALSE, colors=pal, max.words=50, scale=c(8,.3))
findFreqTerms(train_words)
findFreqTerms(dtm)
findFreqTerms(dtm, 50)
library(RWeka)
install.packages("RWeka")
library(RWeka)
inspect(dtm)
?findFreqTerms
findFreqTerms(dtm, 50)
findAssocs(dtm)
findAssocs(dtm, "and")
?findAssocs
findAssocs(dtm, "and", 5)
findAssocs(dtm, "and", .1)
findFreqTerms(dtm, lowfreq=50)
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
tdm <- removeSparseTerms(tdm, 0.995)
inspect(tdm[1:5,1:5])
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
tdm <- removeSparseTerms(tdm, 0.995)
inspect(tdm[1:5,1:5])
heade(tdm)
head(tdm)
tdm
inspect(tdm)
inspect(tdm[1:3])
inspect(tdm)
TrigramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm = TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
tdm
inspect(tdm)
inspect(tdm[1:3])
head(tdm)
tdm
findFreqTerms(tdm, 50)
freq = colSums(as.data.frame(as.matrix(tdm)))
