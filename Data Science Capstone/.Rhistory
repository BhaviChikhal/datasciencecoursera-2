subset(h3_autosomes, signalValue)
h3_autosomes[1]
h3_autosomes[2]
h3_autosomes[3]
h3_autosomes[,1]
h3_autosomes[,2]
h3_autosomes[1,]
h3_autosomes[2,]
h3_autosomes[3,]
h3_autosomes[4,]
h3_autosomes[5,]
h3_autosomes
h3_autosomes
h3
h3_autosomes
h3[[1]]
h3_e003
h3_e003 = h3[[1]]
h3_e003
h3_autosomes = dropSeqlevels(keepStandardChromosomes(h3_e003), c('chrX', 'chrY', 'chrM'))
h3_autosomes
h3_autosomes$SignalValue
h3_autosomes$signalValue
mean(h3_autosomes$signalValue)
h3k4 = query(ah, c("H3K4me3", "H1", "EpigenomeRoadMap", "hg19", "narrowPeak", "E003"))
hh3k4
h3k4
ah = AnnotationHub()
ah = subset(ah, species == "Homo sapiens")
h3k27 = query(ah, c("H3K27me3", "H1", "EpigenomeRoadMap", "hg19", "narrowPeak", "E003"))
library(rtracklayer)
hk3k27_e003 = h3[[1]]
h3k27_autosomes = dropSeqlevels(keepStandardChromosomes(hk3k27_e003), c('chrX', 'chrY', 'chrM'))
h3k27_autosomes
mean(h3k27_autosomes$signalValue)
library(AnnotationHub)
ah = AnnotationHub()
ah = subset(ah, species == "Homo sapiens")
h3k27 = query(ah, c("H3K27me3", "H1", "EpigenomeRoadMap", "hg19", "narrowPeak", "E003"))
library(rtracklayer)
hk3k27_e003 = h3[[1]]
h3k27_autosomes = dropSeqlevels(keepStandardChromosomes(hk3k27_e003), c('chrX', 'chrY', 'chrM'))
mean(h3k27_autosomes$signalValue)
library(AnnotationHub)
ah = AnnotationHub()
ah = subset(ah, species == "Homo sapiens")
h3k27 = query(ah, c("H3K27me3", "H1", "EpigenomeRoadMap", "hg19", "narrowPeak", "E003"))
library(rtracklayer)
hk3k27_e003 = h3k27[[1]]
h3k27_autosomes = dropSeqlevels(keepStandardChromosomes(hk3k27_e003), c('chrX', 'chrY', 'chrM'))
mean(h3k27_autosomes$signalValue)
?findOverlaps
findOverlaps(h3k27_autosomes, h3k4_autosomes)
h3k4_e003 = h3[[1]]
h3k4_autosomes = dropSeqlevels(keepStandardChromosomes(h3_e003), c('chrX', 'chrY', 'chrM'))
findOverlaps(h3k27_autosomes, h3k4_autosomes)
findOverlaps(h3k27_autosomes, h3k4_autosomes)$subjectHits
wow = findOverlaps(h3k27_autosomes, h3k4_autosomes)
wow
wow$subjectHits
rowsum(wow)
as.data.frame(wow)
names(as.data.frame(wow))
as.data.frame(wow)$subjectHits
sum(as.data.frame(wow)$subjectHits)
wow
sum(as.data.frame(wow)$queryHits)
library(dlnm)
library(ggplot2)
library(reshape2)
library(plyr)
data <- chicagoNMMAPS
data1<-data[c(1,7:9,13)]
dmelt = melt(data1, id.vars = 'date')
ggplot(dmelt, aes(x = date, y = value)) +
geom_line() +
theme_bw() +
facet_wrap(~ variable, scales = 'free_y', ncol = 1)+
theme(strip.text.x = element_text(size=14,face="bold"),
strip.background = element_rect(colour="")) +
labs(x = "Date") +
theme(axis.title=element_text(face="bold",size="14"),axis.text=element_text(size=14,face="bold")) +
theme(#panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
strip.background = element_blank(),
panel.border = element_rect(colour = "black"))
install.packages(ggplot2)
install.packages(ggplot)
install.packages('ggplot2')
library(ShortRead)
library(yeastRNASeq)
fastqFilePath <-  system.file("reads", "wt_1_f.fastq.gz", package = "yeastRNASeq")
reads <- readFastq(fastqFilePath)
as(quality(reads), "matrix")[1:2,1:10]
as(sread(reads), "matrix")[1:2,1:10]
as(sread(reads), "data.frame")[1:2,1:10]
as(sread(reads), "data.frame")[1:2]
sread(reads)[1:2]
subseq(sread(reads)[1])
subseq(sread(reads)[1], 5)
subseq(sread(reads)[1], 5,5)
table(subseq(sread(reads)[1], 5,5))
table(subseq(sread(reads)[1:10], 5,5))
table(subseq(sread(reads), 5,5))
363841/length(reads)
as(quality(reads), "matrix")[1:5]
quality(reads)[1]
as(quality(reads), "matrix")[1:5][5]
as(quality(reads), "matrix")[1:10][5]
as(quality(reads), "matrix")[1:2,5:5]
as(quality(reads), "matrix")[1:10,5:5]
as(quality(reads), "matrix")[1:100,5:5]
as(quality(reads), "matrix")[1:length(reads),5:5]
mean(as(quality(reads), "matrix")[1:length(reads),5:5])
library(leeBamViews)
bamFilePath <- system.file("bam", "isowt5_13e.bam", package="leeBamViews")
library(Rsamtools)
seqinfo(bamFilePath)
?leeBamViews
bamFilePath
bamPath <- system.file("extdata", bamFilePath, package="Rsamtools")
bamFile <- BamFile(bamPath)
bamFile
bamFile <- BamFile(bamFilePath)
bamFile
scanBam(bamFile)
aln <- scanBam(bamFile)
names(aln)
length(a;m)
length(aln)
aln[[1]]
aln
names(aln[[1]])
bpaths <- list.files(system.file("bam", package="leeBamViews"), pattern = "bam$", full=TRUE)
bpaths
quickBamFlagSummary(bamFile)
BamViews(bamPath)
names(bamFile)
names(aln)
names(an)
aln <- scanBam(bamFile)
aln <- aln[[1]]
names(aln)
head(aln$pos)
head(aln$mapq)
tail(aln$mapq)
head(aln$cigar)
head(aln$mrnm)
head(aln$mpos)
head(aln$isize)
head(aln$seq)
head(aln$flag)
head(aln$qname)
aln$mrnm
subset(aln, mrnm == "scchr13")
subset(aln, aln$mrnm == "scchr13")
subset(aln, aln$mrnm == "Scchr13")
subset(aln, aln$mrnm == "Scchr13")
?"GRanges"
gr <- GRanges(
ranges = IRanges(start = 800000, end = 801000))
gr <- GRanges(ranges = IRanges(start = 800000, end = 801000))
gr <- GRanges(seqnames = "Scchr13",
ranges = IRanges(start = 800000, end = 801000))
params <- ScanBamParam(which = gr, what = scanBamWhat())
woo <- scanBam(bamFile, param = params)
woo
table(aln$pos)
table(aln$pos > 1)
table(aln$pos = 1)
table(aln$pos == 1)
table(as.data.frame(aln$pos) = 1)
table(as.data.frame(aln$pos) == 1)
table(as.data.frame(aln$pos))
table(table(as.data.frame(aln$pos)))
table(table(as.data.frame(aln$pos)) > 1)
aln <- scanBam(bamFile, param = params)
table(table(as.data.frame(aln$pos)) > 1)
names(aln)
aln
(aln$pos)
aln$$`Scchr13:800000-801000`$pos
aln$`Scchr13:800000-801000`$pos
table(table(as.data.frame(aln$`Scchr13:800000-801000`$pos)) > 1)
table(table(as.data.frame(aln$`Scchr13:800000-801000`$pos)) => 1)
table(table(as.data.frame(aln$`Scchr13:800000-801000`$pos)) >= 1)
gr <- GRanges(seqnames = "Scchr13",
ranges = IRanges(start = 800000, end = 801000))
params <- ScanBamParam(which = gr, what = scanBamWhat())
aln <- scanBam(bamFile, param = params)
table(table(as.data.frame(aln$`Scchr13:800000-801000`$pos)) > 1)''
aln <- scanBam(bamFile)
aln <- aln[[1]]
gr <- GRanges(seqnames = "Scchr13",
ranges = IRanges(start = 800000, end = 801000))
params <- ScanBamParam(which = gr, what = scanBamWhat())
aln <- scanBam(bamFile, param = params)
table(table(as.data.frame(aln$`Scchr13:800000-801000`$pos)) > 1)
aln
getwd()
getwd()
dir()
setwd('Github')
dir
dir()
setwd("datasciencecoursera")
getwd()
dir()
setwd("data science capstone")
#Load Packages
library(tm)
library(RWeka)
#Load Data
dir = 'input/en_US/'
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"))
train_twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
train_news = readLines(con, encoding="UTF-8")
close(con)
#Sample data size due to memory issues
set.seed(100)
train_blogs = sample(train_blogs, 200000)
train_news = sample(train_news, 200000)
train_twitter = sample(train_twitter, 200000)
#Create a corpus from dataset
create_corpus = function(dm){
corpus = Corpus(VectorSource(dm))
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument
return(corpus)
}
#Setup corpus from training data
train_words = c(train_blogs, train_news, train_twitter)
train_corpus = create_corpus(train_words)
install.packages("tm")
install.packages("weka")
install.packages("RWeka")
#Load Packages
library(tm)
library(RWeka)
#Load Data
dir = 'input/en_US/'
train_blogs = readLines(paste0(dir,"en_US.blogs.txt"))
train_twitter = readLines(paste0(dir,"en_US.twitter.txt"))
con = file(paste0(dir,"en_US.news.txt"), open="rb")
train_news = readLines(con, encoding="UTF-8")
close(con)
#Sample data size due to memory issues
set.seed(100)
train_blogs = sample(train_blogs, 200000)
train_news = sample(train_news, 200000)
train_twitter = sample(train_twitter, 200000)
#Create a corpus from dataset
create_corpus = function(dm){
corpus = Corpus(VectorSource(dm))
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, stripWhitespace) # remove whitespaces
corpus = tm_map(corpus, tolower) #lowercase all contents
corpus = tm_map(corpus, removePunctuation) # remove punctuation
corpus = tm_map(corpus, removeWords, c("fuck", "bitch", "ass", "cunt", "pussy", "asshole", "douche")) #remove some swears
corpus = tm_map(corpus, PlainTextDocument) #convert to plaintextdocument
return(corpus)
}
#Setup corpus from training data
train_words = c(train_blogs, train_news, train_twitter)
train_corpus = create_corpus(train_words)
#Create 1gram
ngram1 = DocumentTermMatrix(train_corpus)
ngram1 = as.data.frame(as.matrix(ngram1))
ngram1
wow = as.data.frame(as.matrix(ngram1))
wow = as.data.frame(as.matrix(as.numeric(ngram1)))
wow = as.data.frame((ngram1))
onegramtokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
wow = TermDocumentMatrix(corpus, control = list(tokenize = onegramtokenizer))
onegramtokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
wow = TermDocumentMatrix(train_corpus, control = list(tokenize = onegramtokenizer))
dim(ngram1)
save.image("~/GitHub/datasciencecoursera/Data Science Capstone/ng1v1.RData")
ngram1 = removeSparseTerms(ngram1, 0.9999)
ngram1 = as.data.frame(as.matrix(ngram1))
rm(onegramtokenizer)
as.matrix(ngram1)
ngram1
as.matrix(ngram1)
as.numeric(as.matrix(ngram1))
as.matrix(ngram1)
library("slam")
library(slam)
wow = rollup(ngram1, 1, na.rm=TRUE, FUN = sum)
wow
wow = rollup(ngram1, 2, na.rm=TRUE, FUN = sum)
wo
wow
ngram1
inspect(ngram1)
ngram1 = removeSparseTerms(ngram1, 0.99)
as.matrix(ngram1)
ngram1 = removeSparseTerms(ngram1, 0.99)
ngram1 = as.data.frame(as.matrix(ngram1))
findFreqTerms(ngram1)
findFreqTerms(ngram1, lowfreq=200)
findAssocs(ngram1, "can", corlimit=0.2)
ngram1 = DocumentTermMatrix(train_corpus)
findFreqTerms(ngram1, lowfreq=200)
findAssocs(ngram1, "can", corlimit=0.2)
#Create 2-grams
create_n2 = function(corpus){
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngram2 = TermDocumentMatrix(corpus, control = list(tokenize = TwogramTokenizer))
ngram2 = removeSparseTerms(ngram2, 0.99)
ngram2 = as.data.frame(as.matrix(ngram2))
return(ngram2)
}
ngram2 = create_n2(train_corpus)
save.image("~/GitHub/datasciencecoursera/Data Science Capstone/ng2v2.RData")
#Create 3-grams
create_n3 = function(corpus){
TrigramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ngram3 = TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
ngram3 = as.data.frame(as.matrix(ngram3))
ngram3 = removeSparseTerms(ngram3, 0.99)
return(ngram3)
}
ngram3 = create_n3(train_corpus)
ngram3
TrigramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ngram3 = TermDocumentMatrix(train_corpus, control = list(tokenize = TrigramTokenizer))
save.image("~/GitHub/datasciencecoursera/Data Science Capstone/ng3v2.RData")
ngram1 = removeSparseTerms(ngram1, 0.99)
ngram1 = as.data.frame(as.matrix(ngram1))
save.image("~/GitHub/datasciencecoursera/Data Science Capstone/ng3v2.RData")
ngram3 = removeSparseTerms(ngram3, 0.99)
ngram3 = as.data.frame(as.matrix(ngram3))
ngram3
load("~/GitHub/datasciencecoursera/Data Science Capstone/ng3v2.RData")
ngram2
View(ngram2)
ngram3 = removeSparseTerms(ngram3, 0.999)
ngram3 = as.data.frame(as.matrix(ngram3))
View(ngram3)
#Create 2-grams
create_n2 = function(corpus){
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngram2 = TermDocumentMatrix(corpus, control = list(tokenize = TwogramTokenizer))
ngram2 = removeSparseTerms(ngram2, 0.999)
ngram2 = as.data.frame(as.matrix(ngram2))
return(ngram2)
}
ngram2 = create_n2(train_corpus)
save.image("~/GitHub/datasciencecoursera/Data Science Capstone/ng4v2.RData")
#Process raw ngrams
ngram1_total = colSums(ngram1)
ngram1_raw = as.numeric(ngram1_total[1:525])
ngram1_raw = data.frame(names(ngram1),ngram1_raw)
names(ngram1_raw) = c("words", "total")
ngram1_raw$start = substr(ngram1_raw$words, 1,1)
#Preceding word, after
ngram2_total = rowSums(ngram2)
ngram2_raw = as.numeric(ngram2_total[1:nrow(ngram2)])
ngram2_raw = data.frame(names(ngram2_total),ngram2_raw)
names(ngram2_raw) = c("words", "total")
ngram2_raw$word1 = "dummy"
ngram2_raw$word2 = "dummy"
for(i in 1:nrow(ngram2_raw)){
split_words = unlist(strsplit(as.character(ngram2_raw$words[i]), " "))
ngram2_raw$word1[i] = split_words[1]
ngram2_raw$word2[i] = split_words[2]
}
#Preceding phrase, after
ngram3_total = rowSums(ngram3)
ngram3_raw = as.numeric(ngram3_total[1:nrow(ngram3)])
ngram3_raw = data.frame(names(ngram3_total),ngram3_raw)
names(ngram3_raw) = c("words", "total")
ngram3_raw$phrase = "dummy"
ngram3_raw$word3 = "dummy"
for(i in 1:nrow(ngram3_raw)){
split_words = unlist(strsplit(as.character(ngram3_raw$words[i]), " "))
ngram3_raw$phrase[i] = paste(split_words[1],split_words[2], sep=" ")
ngram3_raw$word3[i] = split_words[3]
}
#Prediction function for n-1(2-3 words total)
pred_word = function(word){
words = as.data.frame(unlist(strsplit(word, " ")))
names(words) = "query"
query_length = nrow(words)
#Nice to have predict completely based on preceding characters
#could be improved by trying to do local alignment
if(query_length == 1){
if(words$query %in% ngram1_raw$words){ #If word in dict, then it is correct
prediction = words[1,1]
}
else if(substr(words$query[1],1,1) %in% ngram1_raw$start){ #Takes the highest occuring same starting char
start_test = substr(words$query[1],1,1)
start_match = ngram1_raw[ngram1_raw$start == start_test,]
prediction = as.vector(start_match$words[which.max(start_match$total)])
}
else{ #Take the highest occuring word, be great if this took it based on local alignment although be less of issue if bigger data size
prediction = as.vector(ngram1_raw$words[which.max(ngram1_raw$total)])
}
}
else if(query_length == 2){
if(words[1,1] %in% ngram2_raw$word1){ #If first word there, take the highest occuring 2nd word following it
if(words[2,1] %in% ngram2_raw$word2){
prediction = paste(words[1,1],words[2,1])
}
else{
word_match = ngram2_raw[ngram2_raw$word1 == words[1,1],]
prediction = paste(words[1,1], as.vector(word_match$word2[which.max(word_match$total)]))
}
}
else{ #Take highest occuring preceding word
word_match = ngram2_raw[ngram2_raw$word1 == as.vector(ngram1_raw$words[which.max(ngram1_raw$total)]),]
if(words[2,1] %in% ngram2_raw$word2){ #Check if in dictionary
prediction = paste(word_match$word1[1],words[2,1])
}
else{ #Second word will be predicted based on highest occurence
prediction = as.vector(ngram2_raw$words[which.max(ngram2_raw$total)])
}
}
}
else if(query_length == 3){
#check if first two words in, if not guess one from list
#check first two words and guess likelihood of next word based on prob
if(paste(words[1,1], words[2,1]) %in% ngram3_raw$phrase){
if(words[3,1] %in% ngram3_raw$word3){
prediction = paste(paste(words[1,1], words[2,1]), words[3,1])
}
else{
word_match = ngram3_raw[ngram3_raw$phrase == paste(words[1,1], words[2,1]),]
prediction = paste(paste(words[1,1], words[2,1]), as.vector(word_match$word3[which.max(word_match$total)]))
}
}
else{ #Take highest occuring phrase
word_match = ngram3_raw[ngram3_raw$phrase == as.vector(ngram2_raw$words[which.max(ngram2_raw$total)]),]
if(words[3,1] %in% ngram3_raw$word3){ #Check if in dictionary
prediction = paste(word_match$phrase[1], words[2,1])
}
else{ #Second word will be predicted based on highest occurence
prediction = paste(word_match$phrase[1], as.vector(word_match$word3[which.max(word_match$total)]))
}
}
}
else{
prediction = "Please limit prediction to 3 works max"
}
return(prediction)
}
save.image("~/GitHub/datasciencecoursera/Data Science Capstone/ng5v2.RData")
load("~/GitHub/datasciencecoursera/Data Science Capstone/ng3v2.RData")
View(ngram1)
w1 = read.csv("shinyapp/data/ng1.csv")
w1
w2 = read.csv("shinyapp/data/ng2.csv")
w3 = read.csv("shinyapp/data/ng3.csv")
w2
w3
w2
w1
#Create 1gram
ngram1 = DocumentTermMatrix(train_corpus)
ngram1 = removeSparseTerms(ngram1, 0.999)
ngram1 = as.data.frame(as.matrix(ngram1))
#Load Packages
library(tm)
library(RWeka)
#Create 1gram
ngram1 = DocumentTermMatrix(train_corpus)
#Create 1gram
ngram1 = DocumentTermMatrix(train_corpus)
ngram1 = removeSparseTerms(ngram1, 0.999)
ngram1 = as.data.frame(as.matrix(ngram1))
#Load Packages
library(tm)
library(RWeka)
#Create 1gram
ngram1 = DocumentTermMatrix(train_corpus)
ngram1 = removeSparseTerms(ngram1, 0.999)
ngram1 = as.data.frame(as.matrix(ngram1))
#Create 2-grams
create_n2 = function(corpus){
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngram2 = TermDocumentMatrix(corpus, control = list(tokenize = TwogramTokenizer))
ngram2 = removeSparseTerms(ngram2, 0.999)
ngram2 = as.data.frame(as.matrix(ngram2))
return(ngram2)
}
ngram2 = create_n2(train_corpus)
rm(ngram2)
rm(ngram2_raw)
rm(ngram3)
rm(ngram3_raw)
rm(ngram2_total)
rm(ngram3_total)
#Load Packages
library(tm)
library(RWeka)
#Create 1gram
ngram1 = DocumentTermMatrix(train_corpus)
ngram1 = removeSparseTerms(ngram1, 0.999)
ngram1 = as.data.frame(as.matrix(ngram1))
load("~/GitHub/datasciencecoursera/Data Science Capstone/ng1v1.RData")
ngram1 = removeSparseTerms(ngram1, 0.999)
ngram1 = as.data.frame(as.matrix(ngram1))
library(tm)
library(RWeka)
ngram1 = removeSparseTerms(ngram1, 0.999)
ngram1 = as.data.frame(as.matrix(ngram1))
